{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_mat_write_to_hdf(matlab_dir: str, excel_path: str, \n",
    "    write_dir: str, flatten: bool = True, split_size: float = .8, seed: int = 42, file_format: str = \"csv\") -> None:\n",
    "    \"\"\"\n",
    "    final function which combines all the other functions to read in \n",
    "    and transform the data.\n",
    "    Returns a train/test split of datasets for further use in modelling.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    matlab_dir -- path to matlab files\n",
    "    excel_path -- path to excel list\n",
    "    write_dir -- path where to write the dataset to\n",
    "    flatten -- Boolean wheter connectivity matrix should be flattend or not <- unused at the Moment!!!\n",
    "    split_size -- the size of the train dataset (default .8)\n",
    "    seed -- pass an int for reproducibility purposes (default 42)\n",
    "    file_format -- str. Pass \"hdf\" for further modelling in python or \"csv\" for R (default \"csv\")\n",
    "    \"\"\"\n",
    "\n",
    "    #load matlab files and excel\n",
    "    res = load_matlab_files(matlab_dir)\n",
    "    delcode_excel = pd.read_excel(excel_path)\n",
    "\n",
    "\n",
    "    #stack matrices\n",
    "    stacked = stack_matrices(res[0])\n",
    "\n",
    "\n",
    "    #creating colnames and merging into one df\n",
    "    colnames = col_names_final_df(delcode_excel, res[0][0].shape[0])\n",
    "    final_df = create_final_df(file_names = res[1], final_columns = colnames,\n",
    "        stacked_matrices = stacked, data_from_excel = delcode_excel)\n",
    "\n",
    "    #create train test splits\n",
    "    train, test = create_train_test_split(data = final_df, split_size = split_size, seed = seed)\n",
    "\n",
    "    write_to_dir(datasets = [train,test], t_direct = write_dir, file_format = file_format)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_matlab_files(directory: str) -> list:\n",
    "    \"\"\"\n",
    "    imports all matlab files from specified directory\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        os.chdir(directory)\n",
    "    except FileNotFoundError:\n",
    "        print(\"invalid path\")\n",
    "        return None\n",
    "    \n",
    "    mat_files_names = os.listdir()\n",
    "    conn_matrices = []\n",
    "    worked = []\n",
    "    \n",
    "    for i in mat_files_names:\n",
    "            \n",
    "        with h5py.File(i, 'r') as f:\n",
    "            conn_matrices.append(np.array(f.get(\"Z\")))\n",
    "            worked.append(i)\n",
    "\n",
    "    \n",
    "    \n",
    "    return conn_matrices, worked\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stack_matrices(matrices: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    this function stacks the connectivity matrices\n",
    "    for the subjects upon each other \n",
    "    so they can be used in a dataframe \n",
    "    \"\"\"\n",
    "    flattened = []\n",
    "    for i in matrices:\n",
    "        #error handling in case one matrix should not work?\n",
    "        flattened.append(flatten_conn_matrix(i))\n",
    "        #error handling for stacking\n",
    "    \n",
    "    return np.stack(flattened, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def flatten_conn_matrix(matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    turns the connectivity matrix into a 1d array\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(matrix, (np.ndarray, np.generic)):\n",
    "        return \"not an ndarray\"\n",
    "    else:\n",
    "        sh = matrix.shape[0]\n",
    "        return matrix[np.triu_indices(sh, k = 1)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def col_names_final_df(data_from_excel: pd.DataFrame, shape: int = 246) -> list:\n",
    "    \"\"\"\n",
    "    creates the columns names for the final data frame \n",
    "    based on shape / number of columns of the used connectivity matrix\n",
    "    \"\"\"\n",
    "    colnames = [\"IDs\"]\n",
    "    colnames = colnames + col_names_conn_matrix(shape)\n",
    "    final_columns = list(data_from_excel.columns) + colnames\n",
    "    return final_columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def col_names_conn_matrix(n: int):\n",
    "    \"\"\"\n",
    "    creates the column names for the flattened connecitvity matrix\n",
    "    \"\"\"\n",
    "    return [str(i) + \"_\" + str(j)  for i in range(1, n+1) for j in range(i+1, n+1)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_final_df(file_names: list, final_columns: list,\n",
    "    stacked_matrices: np.ndarray, data_from_excel: str) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    this function merges the connectivity matrices, the excel and the subject ids\n",
    "    \"\"\"\n",
    "\n",
    "    ids = get_subject_ids(file_names)\n",
    "    ids_added = np.c_[ids, stacked_matrices]\n",
    "\n",
    "    \n",
    "    #final_columns = col_names_final_df(data_from_excel = data_from_excel)\n",
    "    final_df = np.c_[np.array(data_from_excel), ids_added]\n",
    "    final_df = pd.DataFrame(final_df, columns = final_columns)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_subject_ids(file_names: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    gets the subjectIDs if the filenames correspond to the \n",
    "    used format: resultsROI_Subject006_Condition001.mat\n",
    "    would correspond to subject ID 6\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.array([int(i.split(\"Subject\",1)[1][0:3]) for i in file_names])\n",
    "\n",
    "\n",
    "\n",
    "def create_train_test_split(data: pd.DataFrame, split_size: float = .8, seed: int = 42) -> list:\n",
    "    \"\"\"\n",
    "    takes the final data set and splits it into random train and test subsets. \n",
    "    Returns a list containing train-test split of inputs\n",
    "    \n",
    "    Keyword arguments:\n",
    "    data -- dataset to be split into train/test\n",
    "    split_size -- the size of the train dataset (default .8)\n",
    "    seed -- pass an int for reproducibility purposes\n",
    "    \"\"\"\n",
    "    #assert split size between 0 and 1\n",
    "    assert 0 <= split_size <= 1, \"split_size out of bounds\"\n",
    "    \n",
    "    #split into features and target\n",
    "#     features = data.drop('target', axis=1)\n",
    "#     target = data['target']\n",
    "    \n",
    "    #stratify by the target to ensure equal distribution\n",
    "    return train_test_split(data, train_size = split_size, random_state = seed, shuffle = True)\n",
    "\n",
    "def write_to_dir(datasets: list, t_direct: str, file_format: str = \"csv\") -> None:\n",
    "    \"\"\"\n",
    "    writes the list of train/test splits to hdf files for future use in python or csv for future use in R \n",
    "    in the specified directory\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        os.chdir(t_direct)\n",
    "    except FileNotFoundError:\n",
    "        print(\"invalid path\")\n",
    "        return None\n",
    "    \n",
    "    #Gibts ne elegantere LÃ¶sung?\n",
    "    names = [\"train\", \"test\"]\n",
    "    if file_format == \"hdf\":\n",
    "        for i in range(len(datasets)):\n",
    "            datasets[i].to_hdf(names[i] + '.h5', key='df', mode='w')\n",
    "    elif file_format == \"csv\":\n",
    "        for i in range(len(datasets)):\n",
    "            datasets[i].to_csv(names[i] + '.csv', index = False)\n",
    "    else:\n",
    "        print(\"invalid file format selected\")  \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def main():\n",
    "    matlab_dir = r\"...\"\n",
    "    excel_path = r\"...\"\n",
    "    write_dir = r\"...\"\n",
    "\n",
    "\n",
    "    transform_mat_write_to_hdf(matlab_dir = matlab_dir, excel_path = excel_path, \n",
    "    write_dir = write_dir)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matlab_dir = r\"C:\\Users\\Kai\\Desktop\\My Life\\Master\\3. Semester\\Innolabs\\Data\\MatLab\"\n",
    "excel_path = r\"C:\\Users\\Kai\\Desktop\\My Life\\Master\\3. Semester\\Innolabs\\Data\\DELCODE_dataset.xlsx\"\n",
    "write_dir = r\"C:\\Users\\Kai\\Desktop\\My Life\\Master\\3. Semester\\Innolabs\\Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform_mat_write_to_hdf(matlab_dir = matlab_dir, excel_path = excel_path, \n",
    "    write_dir = write_dir, file_format = \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
